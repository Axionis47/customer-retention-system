# Customer Retention System

An end-to-end ML system that reduces customer churn using XGBoost, Reinforcement Learning (PPO), and RLHF (same technique as ChatGPT). Production-ready deployment on Google Cloud Platform.

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.0-red.svg)](https://pytorch.org/)
[![Code Coverage](https://img.shields.io/badge/coverage-90%25-brightgreen.svg)](tests/)
[![GCP](https://img.shields.io/badge/cloud-GCP-blue.svg)](https://cloud.google.com/)

## Overview

This system helps reduce customer churn by deciding when to contact customers and what offers to give them. It uses machine learning to make smart decisions.

**What it does:**
- Predicts which customers might leave (XGBoost with calibration)
- Decides when to contact them and what discount to offer (PPO with constraints)
- Writes personalized messages for each customer (RLHF: SFT â†’ RM â†’ PPO)
- Keeps track of budget and doesn't spam customers (Lagrangian constraints)
- Runs on Google Cloud (Vertex AI, Cloud Run, GCS)

**Key Features:**
- ðŸŽ¯ **3 ML Systems**: XGBoost + PPO + RLHF working together
- ðŸ“Š **Real Data**: 166K+ data points from 4 public datasets
- âœ… **Production-Ready**: 90%+ test coverage, monitoring, CI/CD
- ðŸ’° **Cost-Optimized**: Under $100 to train, ~$10-30/month to run
- ðŸš€ **Scalable**: Handles millions of customers on GCP

## Quick Stats

| Metric | Value |
|--------|-------|
| **Tests** | 46 tests, 90%+ coverage |
| **Training Data** | 166,660 data points |
| **Models** | 6 trained models |
| **Training Cost** | ~$90 |
| **Inference Cost** | ~$10-30/month |
| **Training Time** | ~8 hours |

## Demo

**Input** (customer data):
```json
{
  "customer_id": "C12345",
  "name": "Sam",
  "tenure": 12,
  "monthly_charges": 89.99,
  "contract": "Month-to-month",
  "payment_method": "Electronic check"
}
```

**Output** (retention decision):
```json
{
  "decision": {
    "contact": true,
    "offer_level": 2,
    "offer_percentage": 10,
    "delay_days": 0,
    "estimated_retention_prob": 0.73
  },
  "message": "Hi Sam, we value your 12 months with us! As a thank you, here's a special 10% discount on your next bill. Reply YES to accept.",
  "safety": {
    "passed": true,
    "violations": []
  },
  "metadata": {
    "churn_risk": 0.68,
    "accept_prob": 0.73,
    "expected_value": 45.23
  }
}
```

## How it works

1. Customer data comes in
2. ML models predict if customer will leave
3. System decides: contact now or wait?
4. If contacting, picks best discount offer
5. Writes a personalized message
6. Checks message is safe and appropriate
7. Sends everything back

## Technical Architecture

> **ðŸ“– For detailed architecture with diagrams, see [ARCHITECTURE.md](ARCHITECTURE.md)**

This project combines 3 different ML systems working together:

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Customer Input                               â”‚
â”‚  {tenure, plan, monthly_charges, churn_risk, name, ...}         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYSTEM 1: Tabular Predictors                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Risk Model      â”‚         â”‚  Accept Model    â”‚             â”‚
â”‚  â”‚  (XGBoost)       â”‚         â”‚  (XGBoost)       â”‚             â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚             â”‚
â”‚  â”‚  Input: Customer â”‚         â”‚  Input: Customer â”‚             â”‚
â”‚  â”‚  features        â”‚         â”‚  + Offer level   â”‚             â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚             â”‚
â”‚  â”‚  Output:         â”‚         â”‚  Output:         â”‚             â”‚
â”‚  â”‚  P(churn)        â”‚         â”‚  P(accept|offer) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                              â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SYSTEM 2: PPO Decision Policy                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Retention Environment (Gymnasium)                      â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  State: [churn_risk, accept_probs, budget, cooldown,   â”‚    â”‚
â”‚  â”‚          fatigue, days_since_contact]                   â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Actions: {contact: yes/no, offer: 0-3, delay: 0-7}    â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Reward: revenue_retained - offer_cost - penalties     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  PPO Policy Network (Actor-Critic)                      â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Actor: Ï€(a|s) â†’ action probabilities                   â”‚    â”‚
â”‚  â”‚  Critic: V(s) â†’ state value                             â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Training: Clipped surrogate loss + GAE                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Lagrangian Constraints                                 â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Budget: Î£ offer_cost â‰¤ budget_limit                    â”‚    â”‚
â”‚  â”‚  Fatigue: contacts_per_customer â‰¤ fatigue_cap          â”‚    â”‚
â”‚  â”‚  Cooldown: min_days_between_contacts â‰¥ cooldown        â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Penalty: Î»â‚Â·budget_violation + Î»â‚‚Â·fatigue_violation   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   Decision: Contact?
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                    â”‚           â”‚
                   No          Yes
                    â”‚           â”‚
                    â–¼           â–¼
                  Wait    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  SYSTEM 3: RLHF Message Generator   â”‚
                          â”‚                                     â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                          â”‚  â”‚  SFT Model (OPT-350m)      â”‚    â”‚
                          â”‚  â”‚  + LoRA (r=16, Î±=32)       â”‚    â”‚
                          â”‚  â”‚  + 8-bit quantization      â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  Trained on: OASST1        â”‚    â”‚
                          â”‚  â”‚  (16k conversation pairs)  â”‚    â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                          â”‚               â”‚                     â”‚
                          â”‚               â–¼                     â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                          â”‚  â”‚  Reward Model              â”‚    â”‚
                          â”‚  â”‚  (OPT-350m + LoRA)         â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  Trained on: SHP-2 + HH    â”‚    â”‚
                          â”‚  â”‚  (100k preference pairs)   â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  Loss: Bradley-Terry       â”‚    â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                          â”‚               â”‚                     â”‚
                          â”‚               â–¼                     â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                          â”‚  â”‚  PPO-Text                  â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  Policy: Generate message  â”‚    â”‚
                          â”‚  â”‚  Reward: RM score          â”‚    â”‚
                          â”‚  â”‚  Constraint: KL(Ï€||Ï€_SFT)  â”‚    â”‚
                          â”‚  â”‚              â‰¤ target_KL   â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  Adaptive Î² control        â”‚    â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                          â”‚               â”‚                     â”‚
                          â”‚               â–¼                     â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                          â”‚  â”‚  Safety Shield             â”‚    â”‚
                          â”‚  â”‚                             â”‚    â”‚
                          â”‚  â”‚  âœ“ No banned phrases       â”‚    â”‚
                          â”‚  â”‚  âœ“ Length check            â”‚    â”‚
                          â”‚  â”‚  âœ“ Quiet hours check       â”‚    â”‚
                          â”‚  â”‚  âœ“ Required elements       â”‚    â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                          â”‚               â”‚                     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Final Output                 â”‚
                          â”‚                               â”‚
                          â”‚  {                            â”‚
                          â”‚    decision: {contact, offer} â”‚
                          â”‚    message: "Hi Sam, ..."     â”‚
                          â”‚    safety: {violations: 0}    â”‚
                          â”‚  }                            â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System 1: Tabular Predictors (XGBoost)

**Purpose**: Predict customer behavior

**Models**:
1. **Risk Model** - Predicts probability customer will churn
   - Algorithm: XGBoost classifier
   - Features: tenure, monthly_charges, contract_type, payment_method, etc.
   - Training data: IBM Telco (7,032 customers)
   - Output: P(churn) âˆˆ [0, 1]
   - Calibration: Isotonic regression (ensures probabilities are accurate)
   - Exit criteria: AUC â‰¥ 0.78, ECE â‰¤ 0.05

2. **Accept Model** - Predicts probability customer accepts offer
   - Algorithm: XGBoost classifier
   - Features: customer features + offer_level (0-3)
   - Training data: UCI Bank Marketing (41,188 contacts)
   - Output: P(accept | offer_level) âˆˆ [0, 1]
   - Calibration: Isotonic regression
   - Exit criteria: AUC â‰¥ 0.70, ECE â‰¤ 0.05

**Why XGBoost?**
- Handles tabular data very well
- Fast training and inference
- Built-in feature importance
- Works with missing values

**Calibration**:
```
Raw XGBoost â†’ Isotonic Regression â†’ Calibrated Probabilities
```
This ensures when model says "70% chance", it's actually 70% in real data.

### System 2: PPO Decision Policy (Reinforcement Learning)

**Purpose**: Decide when to contact customers and what offer to give

**Environment** (`RetentionEnv`):
- **State space** (9 dimensions):
  ```
  [churn_risk, accept_prob_0, accept_prob_1, accept_prob_2, accept_prob_3,
   budget_remaining, days_since_contact, cooldown_active, fatigue_count]
  ```

- **Action space** (3 dimensions):
  ```
  contact âˆˆ {0, 1}           # Don't contact or contact
  offer_idx âˆˆ {0, 1, 2, 3}   # Which discount: 0%, 5%, 10%, 15%
  delay âˆˆ {0, 1, ..., 7}     # Days to wait before next decision
  ```

- **Reward function**:
  ```
  reward = revenue_retained - offer_cost - Î»â‚Â·budget_violation - Î»â‚‚Â·fatigue_violation

  where:
    revenue_retained = customer_value Ã— (1 if retained else 0)
    offer_cost = customer_value Ã— offer_percentage
    budget_violation = max(0, total_spent - budget_limit)
    fatigue_violation = max(0, contacts - fatigue_cap)
  ```

**PPO Algorithm**:
```
1. Collect rollouts using current policy Ï€_Î¸
2. Compute advantages using GAE (Î»=0.95):
   A_t = Î£ (Î³Î»)^k Î´_{t+k}
   where Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)

3. Update policy with clipped objective:
   L^CLIP(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
   where r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)

4. Update value function:
   L^VF(Î¸) = E[(V_Î¸(s_t) - V_target)Â²]

5. Add entropy bonus for exploration:
   L(Î¸) = L^CLIP(Î¸) - câ‚Â·L^VF(Î¸) + câ‚‚Â·H(Ï€_Î¸)
```

**Network Architecture**:
```
Input (state) â†’ Shared layers (128 â†’ 128) â†’ Split into 4 heads:
                                              â”œâ”€ Contact head (2 actions)
                                              â”œâ”€ Offer head (4 actions)
                                              â”œâ”€ Delay head (8 actions)
                                              â””â”€ Value head (1 value)
```

**Lagrangian Constraints**:
Instead of hard constraints, we use adaptive penalties:
```
Î»_budget(t+1) = Î»_budget(t) + Î± Â· (budget_used - budget_limit)
Î»_fatigue(t+1) = Î»_fatigue(t) + Î± Â· (contacts - fatigue_cap)
```
This allows the policy to learn to respect constraints automatically.

### System 3: RLHF Message Generator

**Purpose**: Write personalized retention messages

**Three-stage training pipeline**:

#### Stage 1: Supervised Fine-Tuning (SFT)
```
Base Model: facebook/opt-350m (350M parameters)
           â†“
    Add LoRA adapters (r=16, Î±=32)
           â†“
    Train on OASST1 conversations
           â†“
    SFT Model (can generate coherent messages)
```

**LoRA** (Low-Rank Adaptation):
- Instead of fine-tuning all 350M parameters, we add small adapter matrices
- Only train ~0.5M parameters (99.8% reduction!)
- Formula: `W' = W + BA` where B is rÃ—d and A is dÃ—r
- Saves memory and training time

**8-bit Quantization**:
- Store weights in 8-bit instead of 32-bit
- 4x memory reduction
- Minimal accuracy loss

#### Stage 2: Reward Model (RM)
```
Base Model: facebook/opt-350m
           â†“
    Add LoRA adapters
           â†“
    Replace head with reward head (outputs scalar)
           â†“
    Train on preference pairs (chosen vs rejected)
           â†“
    Reward Model (scores message quality)
```

**Training objective** (Bradley-Terry):
```
L = -E[log Ïƒ(r(x, y_chosen) - r(x, y_rejected))]

where:
  r(x, y) = reward model score for prompt x and response y
  Ïƒ = sigmoid function
```

This teaches the model: "chosen message is better than rejected message"

**Training data**:
- SHP-2: Reddit posts with upvotes (60k pairs)
- HH-RLHF: Human preferences from Anthropic (40k pairs)

#### Stage 3: PPO-Text
```
SFT Model â†’ Generate message â†’ Reward Model â†’ Score
     â†‘                                           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Update policy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Objective**:
```
maximize: E[r(x, y)] - Î²Â·KL(Ï€_Î¸ || Ï€_SFT)

where:
  r(x, y) = reward model score
  KL(Ï€_Î¸ || Ï€_SFT) = KL divergence from SFT model
  Î² = adaptive coefficient
```

**Why KL constraint?**
Without it, the model might generate high-reward but nonsensical text.
KL keeps it close to the original SFT model.

**Adaptive Î² control**:
```
if KL > target_KL:
    Î² = Î² Ã— 1.5  # Increase penalty
elif KL < target_KL / 2:
    Î² = Î² / 1.5  # Decrease penalty
```

**Safety Shield**:
After generation, check:
- âœ“ No banned phrases (profanity, false promises)
- âœ“ Length between 50-200 characters
- âœ“ Not during quiet hours (10pm-8am)
- âœ“ Contains required elements (customer name, offer details)

### Data Pipeline

**4 real datasets, 166K+ data points**:

1. **IBM Telco Customer Churn** (Kaggle)
   - 7,032 customers
   - Features: tenure, charges, contract, services
   - Target: Churn (Yes/No)
   - Used for: Risk model training

2. **UCI Bank Marketing**
   - 41,188 contacts
   - Features: age, job, education, campaign history
   - Target: Accepted offer (yes/no)
   - Proxy: offer_level computed from campaign intensity
   - Used for: Accept model training

3. **OASST1** (HuggingFace)
   - 16,440 conversation pairs
   - Format: {prompt, response}
   - Quality: Human-rated conversations
   - Used for: SFT training

4. **SHP-2 + HH-RLHF** (HuggingFace)
   - 100,000 preference pairs
   - Format: {prompt, chosen, rejected}
   - Sources: Reddit upvotes + human feedback
   - Used for: Reward model training

**Processing**:
```
Raw data â†’ Download â†’ Clean â†’ Feature engineering â†’ Train/valid/test split (80/10/10) â†’ Save as Parquet/JSONL
```

All splits use fixed seed (42) for reproducibility.

### Training Infrastructure (GCP)

**Vertex AI Custom Jobs**:
```
Job 1: Risk Model      â†’ n1-standard-4 (CPU)  â†’ 30 min â†’ $2
Job 2: Accept Model    â†’ n1-standard-4 (CPU)  â†’ 30 min â†’ $3
Job 3: SFT Model       â†’ g2-standard-4 (L4)   â†’ 2 hrs  â†’ $30
Job 4: RM Model        â†’ g2-standard-4 (L4)   â†’ 1 hr   â†’ $15
Job 5: PPO-Text        â†’ g2-standard-4 (L4)   â†’ 2 hrs  â†’ $30
Job 6: PPO-Decision    â†’ n1-standard-4 (CPU)  â†’ 1 hr   â†’ $10
                                                Total: ~$90
```

**Docker Image**:
- Base: `nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04`
- Python 3.11
- PyTorch 2.2.0 with CUDA 12.1
- All dependencies from `pyproject.toml`

**Storage**:
- `gs://plotpointe-churn-data/` - Processed datasets
- `gs://plotpointe-churn-models/` - Trained models and configs

**Job Dependencies**:
```
Risk Model â”€â”
            â”œâ”€â†’ PPO Decision
Accept Modelâ”˜

SFT Model â”€â”
           â”œâ”€â†’ PPO Text
RM Model â”€â”€â”˜
```

Jobs wait for dependencies before starting.

### Evaluation Metrics

**Tabular Models**:
- AUC-ROC: Area under ROC curve (discrimination)
- ECE: Expected Calibration Error (calibration quality)
- Precision/Recall at different thresholds

**PPO Decision**:
- NRR: Net Revenue Retention
- ROI: Return on Investment
- Constraint violations: Budget and fatigue
- Contact rate: % of customers contacted

**RLHF**:
- Win rate: % of times PPO-text beats baseline
- KL divergence: Distance from SFT model
- Safety violations: % of unsafe messages
- Human evaluation: Quality ratings

### What Makes This Hard?

1. **Multi-objective optimization**: Maximize retention, minimize cost, respect constraints
2. **Sequential decisions**: When to contact affects future opportunities
3. **Exploration vs exploitation**: Try new strategies vs use known good ones
4. **Constraint satisfaction**: Hard limits on budget and contact frequency
5. **Text generation quality**: Must be helpful, safe, and personalized
6. **Calibration**: Probabilities must be accurate for good decisions
7. **Production deployment**: Must handle real traffic, scale, and fail gracefully

## What you need

- Python 3.11
- Kaggle account (for downloading datasets)
- Docker (if you want to test locally)
- Google Cloud account (if you want to deploy)

## Quick Start

### Step 1: Get Kaggle API key

1. Go to https://www.kaggle.com/settings/account
2. Click "Create New API Token"
3. Save the file to `~/.kaggle/kaggle.json`

```bash
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json
```

### Step 2: Install everything

```bash
make setup
```

### Step 3: Download data

```bash
make data.all
```

This downloads 4 datasets:
- IBM Telco Customer Churn (7,000 customers)
- UCI Bank Marketing (41,000 customers)
- OASST1 conversations (16,000 pairs)
- Human preference data (100,000 pairs)

### Step 4: Run tests

```bash
make test
```

### Step 5: Start the API

```bash
make serve
```

API will run at http://localhost:8080

### Step 6: Test it

```bash
curl http://localhost:8080/healthz

curl -X POST http://localhost:8080/retain \
  -H "Content-Type: application/json" \
  -d '{
    "customer_facts": {
      "tenure": 17,
      "plan": "Pro",
      "churn_risk": 0.65,
      "name": "Sam"
    }
  }'
```

You'll get back:
```json
{
  "decision": {
    "contact": true,
    "offer_level": 2,
    "followup_days": 7
  },
  "message": "Hi Sam, thanks for being with us..."
}
```

## Training Models

If you want to train models yourself:

```bash
# Train churn prediction model
make train-risk

# Train offer acceptance model
make train-accept

# Train message generation models
make train-sft
make train-rm
make train-ppo-text

# Train decision policy
make train-ppo
```

## Deploying to Google Cloud

Set these environment variables:

```bash
export GCP_PROJECT_ID="your-project-id"
export GCP_REGION="us-central1"
```

Then run:

```bash
cd ops/terraform
terraform init
terraform apply
```

This sets up:
- Storage buckets
- Docker registry
- Service accounts
- Cloud Run service

Deploy the API:

```bash
make deploy
```

## Project Structure

```
â”œâ”€â”€ agents/          # PPO policy and baselines
â”œâ”€â”€ data/            # Data processors and catalog
â”œâ”€â”€ env/             # Retention environment
â”œâ”€â”€ eval/            # Evaluation metrics and tests
â”œâ”€â”€ models/          # Risk and acceptance models
â”œâ”€â”€ rlhf/            # SFT, RM, PPO-text training
â”œâ”€â”€ serve/           # FastAPI application
â”œâ”€â”€ ops/             # Deployment configs and scripts
â””â”€â”€ tests/           # All tests
```

## What's Inside

**Models trained:**
- Churn risk predictor (XGBoost)
- Offer acceptance predictor (XGBoost)
- Message generator (OPT-350m fine-tuned)
- Reward model (for rating messages)
- Decision policy (PPO with constraints)

**Datasets used:**
- IBM Telco (7,000 customers)
- UCI Bank Marketing (41,000 customers)
- OASST1 conversations (16,000 pairs)
- Human preferences (100,000 pairs)

**Tests:**
- 46 tests total
- 90%+ code coverage
- Unit, integration, contract, and e2e tests
## Common Issues

**Tests fail:**
```bash
make setup
```

**Docker build fails:**
Check that `.dockerignore` excludes `.venv` folder

**API returns errors:**
```bash
# Check logs
gcloud logging read "resource.type=cloud_run_revision" --limit 50

# Check if models loaded
curl https://your-service-url/readyz
```

## Cost

Training all models: ~$90 (one-time)
Running API: ~$10-30/month (low traffic)

## Project Achievements

### Technical Complexity
âœ… **Implemented 3 Advanced ML Techniques**
- XGBoost with isotonic calibration (AUC â‰¥ 0.78, ECE â‰¤ 0.05)
- PPO from scratch with GAE, clipped surrogate loss, Lagrangian constraints
- Complete RLHF pipeline (SFT â†’ Reward Model â†’ PPO-Text)

âœ… **Production-Grade Engineering**
- 90%+ test coverage (46 tests: unit, integration, e2e)
- CI/CD pipeline with automated testing and deployment
- Infrastructure as code (Terraform)
- Monitoring, logging, and alerting
- Docker with CUDA support

âœ… **Real Data, Not Synthetic**
- 166,660 data points from 4 public datasets
- Automated data pipeline (download â†’ process â†’ validate â†’ upload)
- Proper train/valid/test splits with reproducibility

âœ… **Cost-Optimized**
- Training: ~$90 (under budget)
- Inference: ~$10-30/month
- Auto-scaling to zero when not used

### What This Demonstrates

**For ML Engineers:**
- Can implement research papers (PPO, RLHF, Bradley-Terry loss)
- Understand deep RL (policy gradients, advantage estimation, KL constraints)
- Know how to calibrate models (isotonic regression)
- Can build custom RL environments (Gymnasium)

**For Software Engineers:**
- Write clean, tested code (90%+ coverage)
- Design modular architectures
- Build production APIs (FastAPI)
- Set up CI/CD pipelines

**For Data Engineers:**
- Build automated data pipelines
- Handle multiple data sources (Kaggle, HuggingFace, UCI)
- Ensure data quality and reproducibility

**For DevOps Engineers:**
- Deploy to GCP (Vertex AI, Cloud Run, GCS)
- Use infrastructure as code (Terraform)
- Set up monitoring and alerting
- Optimize costs

### Why This Is Hard

1. **Multi-objective optimization**: Balance retention, cost, and constraints simultaneously
2. **Sequential decision-making**: Actions affect future opportunities (can't just optimize greedily)
3. **Constraint satisfaction**: Hard limits on budget and contact frequency must be respected
4. **Text generation quality**: Messages must be helpful, safe, and personalized
5. **Production deployment**: Must handle real traffic, scale, and fail gracefully
6. **Research implementation**: Implementing algorithms from papers (not just using libraries)

### Unique Aspects

ðŸŽ¯ **Combines 3 Advanced Techniques** - Most projects use one ML technique. This uses XGBoost, PPO, and RLHF together.

ðŸ­ **Production-Ready** - Not a tutorial or notebook. Real deployment with testing, monitoring, and CI/CD.

ðŸ“Š **Real Data** - 166K+ data points from public datasets, not synthetic data.

ðŸ”¬ **Research-Level** - Implemented PPO and RLHF from papers, not just using high-level libraries.

ðŸŽ“ **End-to-End** - Data pipeline â†’ Training â†’ Evaluation â†’ Deployment. Complete ownership.

## Project Structure

```
.
â”œâ”€â”€ agents/              # PPO policy implementations
â”œâ”€â”€ data/                # Data processors and catalog
â”œâ”€â”€ env/                 # Custom RL environment
â”œâ”€â”€ models/              # XGBoost risk/accept models
â”œâ”€â”€ rlhf/                # SFT, RM, PPO-text training
â”œâ”€â”€ serve/               # FastAPI serving layer
â”œâ”€â”€ ops/                 # Deployment (Docker, Terraform, scripts)
â”œâ”€â”€ tests/               # 46 tests (90%+ coverage)
â”œâ”€â”€ Makefile             # Automation commands
â””â”€â”€ README.md            # This file
```

## Technologies Used

**Languages**: Python 3.11

**ML/DL**: PyTorch 2.2.0, Transformers, XGBoost, scikit-learn, PEFT (LoRA)

**Cloud**: GCP (Vertex AI, Cloud Run, GCS, Cloud Build, Artifact Registry)

**Infrastructure**: Docker, Terraform, GitHub Actions

**API**: FastAPI, Pydantic, Uvicorn

**Data**: Pandas, Datasets (HuggingFace), Kaggle API

**Testing**: Pytest, Coverage.py

**Tools**: Make, Git, gcloud CLI

## Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Complete system architecture with Mermaid diagrams
- **[PROJECT_HIGHLIGHTS.md](PROJECT_HIGHLIGHTS.md)** - Key achievements and skills demonstrated
- **[QUICKSTART.md](QUICKSTART.md)** - Quick setup guide
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Deployment instructions
- **[TRAINING_PLAYBOOK.md](TRAINING_PLAYBOOK.md)** - Training guide
- **[DATA_PIPELINE.md](DATA_PIPELINE.md)** - Data processing details

## Notes

- All random seeds set to 42 for reproducibility
- Models use 8-bit quantization to save memory
- API scales to zero when not used
- Safety checks run on all generated messages

---

**Built with**: Python, PyTorch, XGBoost, FastAPI, GCP, Docker, Terraform
